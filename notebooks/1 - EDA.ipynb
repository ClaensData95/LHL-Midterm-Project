{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The os module has a perfect method to list files in a directory.\n",
    "- Pandas json normalize could work here but is not necessary to convert the JSON data to a dataframe.\n",
    "- You may need a nested for-loop to access each sale!\n",
    "- We've put a lot of time into creating the structure of this repository, and it's a good example for future projects.  In the file functions_variables.py, there is an example function that you can import and use.  If you have any variables, functions or classes that you want to make, they can be put in the functions_variables.py file and imported into a notebook.  Note that only .py files can be imported into a notebook. If you want to import everything from a .py file, you can use the following:\n",
    "```python\n",
    "from functions_variables import *\n",
    "```\n",
    "If you just import functions_variables, then each object from the file will need to be prepended with \"functions_variables\"\\\n",
    "Using this .py file will keep your notebooks very organized and make it easier to reuse code between notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this is not an exhaustive list of libraries)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one file first to see wha# Folder containing all the JSON files\n",
    "json_folder = '/home/t0si/LHL-Midterm-Project/data/housing'\n",
    "\n",
    "# List to hold individual DataFrames\n",
    "data_frames = []\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: IL_Springfield_3.json\n",
      "File IL_Springfield_3.json processed. Rows: 34\n",
      "Processing file: SC_Columbia_3.json\n",
      "File SC_Columbia_3.json processed. Rows: 31\n",
      "Processing file: WY_Cheyenne_2.json\n",
      "No valid data in WY_Cheyenne_2.json. Skipping...\n",
      "Processing file: KY_Frankfort_2.json\n",
      "File KY_Frankfort_2.json processed. Rows: 42\n",
      "Processing file: WY_Cheyenne_3.json\n",
      "No valid data in WY_Cheyenne_3.json. Skipping...\n",
      "Processing file: VT_Montpelier_2.json\n",
      "No valid data in VT_Montpelier_2.json. Skipping...\n",
      "Processing file: NJ_Trenton_4.json\n",
      "File NJ_Trenton_4.json processed. Rows: 42\n",
      "Processing file: VA_Richmond_4.json\n",
      "File VA_Richmond_4.json processed. Rows: 42\n",
      "Processing file: OK_OklahomaCity_1.json\n",
      "File OK_OklahomaCity_1.json processed. Rows: 42\n",
      "Processing file: FL_Tallahassee_2.json\n",
      "File FL_Tallahassee_2.json processed. Rows: 42\n",
      "Processing file: WA_Olympia_4.json\n",
      "File WA_Olympia_4.json processed. Rows: 42\n",
      "Processing file: MN_St.Paul_2.json\n",
      "File MN_St.Paul_2.json processed. Rows: 42\n",
      "Processing file: AL_Montgomery_4.json\n",
      "File AL_Montgomery_4.json processed. Rows: 42\n",
      "Processing file: TN_Nashville_0.json\n",
      "File TN_Nashville_0.json processed. Rows: 18\n",
      "Processing file: ND_Bismarck_3.json\n",
      "No valid data in ND_Bismarck_3.json. Skipping...\n",
      "Processing file: AL_Montgomery_3.json\n",
      "File AL_Montgomery_3.json processed. Rows: 42\n",
      "Processing file: LA_BatonRouge_4.json\n",
      "File LA_BatonRouge_4.json processed. Rows: 42\n",
      "Processing file: TX_Austin_2.json\n",
      "File TX_Austin_2.json processed. Rows: 42\n",
      "Processing file: UT_SaltLakeCity_3.json\n",
      "File UT_SaltLakeCity_3.json processed. Rows: 42\n",
      "Processing file: PA_Harrisburg_4.json\n",
      "File PA_Harrisburg_4.json processed. Rows: 42\n",
      "Processing file: DE_Dover_4.json\n",
      "File DE_Dover_4.json processed. Rows: 42\n",
      "Processing file: NH_Concord_2.json\n",
      "File NH_Concord_2.json processed. Rows: 1\n",
      "Processing file: DE_Dover_3.json\n",
      "File DE_Dover_3.json processed. Rows: 42\n",
      "Processing file: CT_Hartford_3.json\n",
      "File CT_Hartford_3.json processed. Rows: 42\n",
      "Processing file: UT_SaltLakeCity_1.json\n",
      "File UT_SaltLakeCity_1.json processed. Rows: 42\n",
      "Processing file: MS_Jackson_3.json\n",
      "No valid data in MS_Jackson_3.json. Skipping...\n",
      "Processing file: MO_JeffersonCity_0.json\n",
      "File MO_JeffersonCity_0.json processed. Rows: 21\n",
      "Processing file: OH_Columbus_1.json\n",
      "File OH_Columbus_1.json processed. Rows: 42\n",
      "Processing file: NJ_Trenton_0.json\n",
      "File NJ_Trenton_0.json processed. Rows: 42\n",
      "Processing file: CA_Sacramento_4.json\n",
      "File CA_Sacramento_4.json processed. Rows: 42\n",
      "Processing file: CT_Hartford_1.json\n",
      "File CT_Hartford_1.json processed. Rows: 42\n",
      "Processing file: HI_Honolulu_3.json\n",
      "No valid data in HI_Honolulu_3.json. Skipping...\n",
      "Processing file: DE_Dover_2.json\n",
      "File DE_Dover_2.json processed. Rows: 42\n",
      "Processing file: NV_CarsonCity_4.json\n",
      "File NV_CarsonCity_4.json processed. Rows: 42\n",
      "Processing file: MA_Boston_4.json\n",
      "File MA_Boston_4.json processed. Rows: 42\n",
      "Processing file: NE_Lincoln_4.json\n",
      "File NE_Lincoln_4.json processed. Rows: 42\n",
      "Processing file: TX_Austin_0.json\n",
      "File TX_Austin_0.json processed. Rows: 42\n",
      "Processing file: WV_Charleston_4.json\n",
      "File WV_Charleston_4.json processed. Rows: 39\n",
      "Processing file: FL_Tallahassee_3.json\n",
      "File FL_Tallahassee_3.json processed. Rows: 42\n",
      "Processing file: RI_Providence_2.json\n",
      "File RI_Providence_2.json processed. Rows: 42\n",
      "Processing file: OR_Salem_0.json\n",
      "File OR_Salem_0.json processed. Rows: 42\n",
      "Processing file: ND_Bismarck_1.json\n",
      "File ND_Bismarck_1.json processed. Rows: 1\n",
      "Processing file: NV_CarsonCity_3.json\n",
      "File NV_CarsonCity_3.json processed. Rows: 42\n",
      "Processing file: IL_Springfield_1.json\n",
      "File IL_Springfield_1.json processed. Rows: 36\n",
      "Processing file: IN_Indianapolis_4.json\n",
      "File IN_Indianapolis_4.json processed. Rows: 42\n",
      "Processing file: NE_Lincoln_0.json\n",
      "File NE_Lincoln_0.json processed. Rows: 42\n",
      "Processing file: SC_Columbia_4.json\n",
      "File SC_Columbia_4.json processed. Rows: 30\n",
      "Processing file: IA_DesMoines_2.json\n",
      "File IA_DesMoines_2.json processed. Rows: 42\n",
      "Processing file: OR_Salem_2.json\n",
      "File OR_Salem_2.json processed. Rows: 42\n",
      "Processing file: KY_Frankfort_0.json\n",
      "File KY_Frankfort_0.json processed. Rows: 42\n",
      "Processing file: MA_Boston_2.json\n",
      "File MA_Boston_2.json processed. Rows: 42\n",
      "Processing file: OK_OklahomaCity_0.json\n",
      "File OK_OklahomaCity_0.json processed. Rows: 42\n",
      "Processing file: MO_JeffersonCity_4.json\n",
      "File MO_JeffersonCity_4.json processed. Rows: 17\n",
      "Processing file: KY_Frankfort_4.json\n",
      "File KY_Frankfort_4.json processed. Rows: 42\n",
      "Processing file: FL_Tallahassee_0.json\n",
      "File FL_Tallahassee_0.json processed. Rows: 42\n",
      "Processing file: OK_OklahomaCity_3.json\n",
      "File OK_OklahomaCity_3.json processed. Rows: 42\n",
      "Processing file: DE_Dover_0.json\n",
      "File DE_Dover_0.json processed. Rows: 42\n",
      "Processing file: RI_Providence_3.json\n",
      "File RI_Providence_3.json processed. Rows: 42\n",
      "Processing file: OK_OklahomaCity_2.json\n",
      "File OK_OklahomaCity_2.json processed. Rows: 42\n",
      "Processing file: PA_Harrisburg_0.json\n",
      "File PA_Harrisburg_0.json processed. Rows: 42\n",
      "Processing file: AZ_Phoenix_4.json\n",
      "File AZ_Phoenix_4.json processed. Rows: 42\n",
      "Processing file: NV_CarsonCity_0.json\n",
      "File NV_CarsonCity_0.json processed. Rows: 42\n",
      "Processing file: WA_Olympia_2.json\n",
      "File WA_Olympia_2.json processed. Rows: 42\n",
      "Processing file: MD_Annapolis_1.json\n",
      "File MD_Annapolis_1.json processed. Rows: 42\n",
      "Processing file: CO_Denver_2.json\n",
      "File CO_Denver_2.json processed. Rows: 42\n",
      "Processing file: TN_Nashville_4.json\n",
      "File TN_Nashville_4.json processed. Rows: 14\n",
      "Processing file: ME_Augusta_1.json\n",
      "No valid data in ME_Augusta_1.json. Skipping...\n",
      "Processing file: VT_Montpelier_4.json\n",
      "No valid data in VT_Montpelier_4.json. Skipping...\n",
      "Processing file: UT_SaltLakeCity_0.json\n",
      "File UT_SaltLakeCity_0.json processed. Rows: 42\n",
      "Processing file: MS_Jackson_2.json\n",
      "No valid data in MS_Jackson_2.json. Skipping...\n",
      "Processing file: GA_Atlanta_2.json\n",
      "File GA_Atlanta_2.json processed. Rows: 42\n",
      "Processing file: IA_DesMoines_3.json\n",
      "File IA_DesMoines_3.json processed. Rows: 42\n",
      "Processing file: MT_Helena_4.json\n",
      "File MT_Helena_4.json processed. Rows: 42\n",
      "Processing file: WI_Madison_0.json\n",
      "File WI_Madison_0.json processed. Rows: 42\n",
      "Processing file: MD_Annapolis_4.json\n",
      "File MD_Annapolis_4.json processed. Rows: 42\n",
      "Processing file: NH_Concord_1.json\n",
      "File NH_Concord_1.json processed. Rows: 2\n",
      "Processing file: NE_Lincoln_2.json\n",
      "File NE_Lincoln_2.json processed. Rows: 42\n",
      "Processing file: MN_St.Paul_1.json\n",
      "File MN_St.Paul_1.json processed. Rows: 42\n",
      "Processing file: WI_Madison_1.json\n",
      "File WI_Madison_1.json processed. Rows: 42\n",
      "Processing file: AK_Juneau_1.json\n",
      "File AK_Juneau_1.json processed. Rows: 7\n",
      "Processing file: AL_Montgomery_1.json\n",
      "File AL_Montgomery_1.json processed. Rows: 42\n",
      "Processing file: NE_Lincoln_1.json\n",
      "File NE_Lincoln_1.json processed. Rows: 42\n",
      "Processing file: OK_OklahomaCity_4.json\n",
      "File OK_OklahomaCity_4.json processed. Rows: 42\n",
      "Processing file: PA_Harrisburg_2.json\n",
      "File PA_Harrisburg_2.json processed. Rows: 42\n",
      "Processing file: NH_Concord_4.json\n",
      "No valid data in NH_Concord_4.json. Skipping...\n",
      "Processing file: NJ_Trenton_3.json\n",
      "File NJ_Trenton_3.json processed. Rows: 42\n",
      "Processing file: MI_Lansing_1.json\n",
      "File MI_Lansing_1.json processed. Rows: 42\n",
      "Processing file: MD_Annapolis_0.json\n",
      "File MD_Annapolis_0.json processed. Rows: 42\n",
      "Processing file: MT_Helena_1.json\n",
      "File MT_Helena_1.json processed. Rows: 42\n",
      "Processing file: OH_Columbus_3.json\n",
      "File OH_Columbus_3.json processed. Rows: 42\n",
      "Processing file: ME_Augusta_4.json\n",
      "No valid data in ME_Augusta_4.json. Skipping...\n",
      "Processing file: ID_Boise_2.json\n",
      "File ID_Boise_2.json processed. Rows: 42\n",
      "Processing file: MS_Jackson_4.json\n",
      "No valid data in MS_Jackson_4.json. Skipping...\n",
      "Processing file: MO_JeffersonCity_2.json\n",
      "File MO_JeffersonCity_2.json processed. Rows: 19\n",
      "Processing file: TN_Nashville_2.json\n",
      "File TN_Nashville_2.json processed. Rows: 16\n",
      "Processing file: KY_Frankfort_1.json\n",
      "File KY_Frankfort_1.json processed. Rows: 42\n",
      "Processing file: WI_Madison_2.json\n",
      "File WI_Madison_2.json processed. Rows: 42\n",
      "Processing file: NY_Albany_3.json\n",
      "File NY_Albany_3.json processed. Rows: 42\n",
      "Processing file: KS_Topeka_2.json\n",
      "File KS_Topeka_2.json processed. Rows: 10\n",
      "Processing file: AZ_Phoenix_1.json\n",
      "File AZ_Phoenix_1.json processed. Rows: 42\n",
      "Processing file: AZ_Phoenix_0.json\n",
      "File AZ_Phoenix_0.json processed. Rows: 42\n",
      "Processing file: SD_Pierre_1.json\n",
      "No valid data in SD_Pierre_1.json. Skipping...\n",
      "Processing file: AR_LittleRock_3.json\n",
      "File AR_LittleRock_3.json processed. Rows: 42\n",
      "Processing file: NY_Albany_4.json\n",
      "File NY_Albany_4.json processed. Rows: 42\n",
      "Processing file: AK_Juneau_0.json\n",
      "File AK_Juneau_0.json processed. Rows: 8\n",
      "Processing file: WY_Cheyenne_4.json\n",
      "No valid data in WY_Cheyenne_4.json. Skipping...\n",
      "Processing file: MO_JeffersonCity_1.json\n",
      "File MO_JeffersonCity_1.json processed. Rows: 20\n",
      "Processing file: WA_Olympia_0.json\n",
      "File WA_Olympia_0.json processed. Rows: 42\n",
      "Processing file: WV_Charleston_3.json\n",
      "File WV_Charleston_3.json processed. Rows: 40\n",
      "Processing file: TN_Nashville_1.json\n",
      "File TN_Nashville_1.json processed. Rows: 17\n",
      "Processing file: NM_SantaFe_2.json\n",
      "File NM_SantaFe_2.json processed. Rows: 42\n",
      "Processing file: MN_St.Paul_4.json\n",
      "File MN_St.Paul_4.json processed. Rows: 42\n",
      "Processing file: VT_Montpelier_1.json\n",
      "No valid data in VT_Montpelier_1.json. Skipping...\n",
      "Processing file: AK_Juneau_4.json\n",
      "File AK_Juneau_4.json processed. Rows: 4\n",
      "Processing file: IL_Springfield_0.json\n",
      "File IL_Springfield_0.json processed. Rows: 37\n",
      "Processing file: CA_Sacramento_3.json\n",
      "File CA_Sacramento_3.json processed. Rows: 42\n",
      "Processing file: SD_Pierre_4.json\n",
      "No valid data in SD_Pierre_4.json. Skipping...\n",
      "Processing file: SD_Pierre_3.json\n",
      "No valid data in SD_Pierre_3.json. Skipping...\n",
      "Processing file: KS_Topeka_4.json\n",
      "File KS_Topeka_4.json processed. Rows: 8\n",
      "Processing file: UT_SaltLakeCity_4.json\n",
      "File UT_SaltLakeCity_4.json processed. Rows: 42\n",
      "Processing file: AR_LittleRock_0.json\n",
      "File AR_LittleRock_0.json processed. Rows: 42\n",
      "Processing file: KS_Topeka_0.json\n",
      "File KS_Topeka_0.json processed. Rows: 12\n",
      "Processing file: AK_Juneau_2.json\n",
      "File AK_Juneau_2.json processed. Rows: 6\n",
      "Processing file: AR_LittleRock_1.json\n",
      "File AR_LittleRock_1.json processed. Rows: 42\n",
      "Processing file: GA_Atlanta_3.json\n",
      "File GA_Atlanta_3.json processed. Rows: 42\n",
      "Processing file: TN_Nashville_3.json\n",
      "File TN_Nashville_3.json processed. Rows: 15\n",
      "Processing file: WV_Charleston_0.json\n",
      "File WV_Charleston_0.json processed. Rows: 42\n",
      "Processing file: SC_Columbia_2.json\n",
      "File SC_Columbia_2.json processed. Rows: 32\n",
      "Processing file: OR_Salem_1.json\n",
      "File OR_Salem_1.json processed. Rows: 42\n",
      "Processing file: KY_Frankfort_3.json\n",
      "File KY_Frankfort_3.json processed. Rows: 42\n",
      "Processing file: MD_Annapolis_3.json\n",
      "File MD_Annapolis_3.json processed. Rows: 42\n",
      "Processing file: NM_SantaFe_1.json\n",
      "File NM_SantaFe_1.json processed. Rows: 42\n",
      "Processing file: ME_Augusta_2.json\n",
      "No valid data in ME_Augusta_2.json. Skipping...\n",
      "Processing file: VT_Montpelier_0.json\n",
      "No valid data in VT_Montpelier_0.json. Skipping...\n",
      "Processing file: GA_Atlanta_1.json\n",
      "File GA_Atlanta_1.json processed. Rows: 42\n",
      "Processing file: NC_Raleigh_3.json\n",
      "File NC_Raleigh_3.json processed. Rows: 42\n",
      "Processing file: CO_Denver_1.json\n",
      "File CO_Denver_1.json processed. Rows: 42\n",
      "Processing file: LA_BatonRouge_1.json\n",
      "File LA_BatonRouge_1.json processed. Rows: 42\n",
      "Processing file: NY_Albany_1.json\n",
      "File NY_Albany_1.json processed. Rows: 42\n",
      "Processing file: RI_Providence_0.json\n",
      "File RI_Providence_0.json processed. Rows: 42\n",
      "Processing file: CA_Sacramento_2.json\n",
      "File CA_Sacramento_2.json processed. Rows: 42\n",
      "Processing file: PA_Harrisburg_1.json\n",
      "File PA_Harrisburg_1.json processed. Rows: 42\n",
      "Processing file: ND_Bismarck_2.json\n",
      "No valid data in ND_Bismarck_2.json. Skipping...\n",
      "Processing file: PA_Harrisburg_3.json\n",
      "File PA_Harrisburg_3.json processed. Rows: 42\n",
      "Processing file: WY_Cheyenne_1.json\n",
      "No valid data in WY_Cheyenne_1.json. Skipping...\n",
      "Processing file: NC_Raleigh_0.json\n",
      "File NC_Raleigh_0.json processed. Rows: 42\n",
      "Processing file: ME_Augusta_0.json\n",
      "No valid data in ME_Augusta_0.json. Skipping...\n",
      "Processing file: CO_Denver_3.json\n",
      "File CO_Denver_3.json processed. Rows: 42\n",
      "Processing file: NV_CarsonCity_2.json\n",
      "File NV_CarsonCity_2.json processed. Rows: 42\n",
      "Processing file: GA_Atlanta_4.json\n",
      "File GA_Atlanta_4.json processed. Rows: 42\n",
      "Processing file: LA_BatonRouge_0.json\n",
      "File LA_BatonRouge_0.json processed. Rows: 42\n",
      "Processing file: FL_Tallahassee_1.json\n",
      "File FL_Tallahassee_1.json processed. Rows: 42\n",
      "Processing file: VA_Richmond_2.json\n",
      "File VA_Richmond_2.json processed. Rows: 42\n",
      "Processing file: ID_Boise_3.json\n",
      "File ID_Boise_3.json processed. Rows: 42\n",
      "Processing file: IN_Indianapolis_0.json\n",
      "File IN_Indianapolis_0.json processed. Rows: 42\n",
      "Processing file: NC_Raleigh_2.json\n",
      "File NC_Raleigh_2.json processed. Rows: 42\n",
      "Processing file: MT_Helena_2.json\n",
      "File MT_Helena_2.json processed. Rows: 42\n",
      "Processing file: NE_Lincoln_3.json\n",
      "File NE_Lincoln_3.json processed. Rows: 42\n",
      "Processing file: MI_Lansing_4.json\n",
      "File MI_Lansing_4.json processed. Rows: 42\n",
      "Processing file: SD_Pierre_0.json\n",
      "No valid data in SD_Pierre_0.json. Skipping...\n",
      "Processing file: IN_Indianapolis_3.json\n",
      "File IN_Indianapolis_3.json processed. Rows: 42\n",
      "Processing file: ME_Augusta_3.json\n",
      "No valid data in ME_Augusta_3.json. Skipping...\n",
      "Processing file: CA_Sacramento_1.json\n",
      "File CA_Sacramento_1.json processed. Rows: 42\n",
      "Processing file: DE_Dover_1.json\n",
      "File DE_Dover_1.json processed. Rows: 42\n",
      "Processing file: WV_Charleston_1.json\n",
      "File WV_Charleston_1.json processed. Rows: 42\n",
      "Processing file: MN_St.Paul_3.json\n",
      "File MN_St.Paul_3.json processed. Rows: 42\n",
      "Processing file: IL_Springfield_4.json\n",
      "File IL_Springfield_4.json processed. Rows: 33\n",
      "Processing file: MA_Boston_1.json\n",
      "File MA_Boston_1.json processed. Rows: 42\n",
      "Processing file: CT_Hartford_0.json\n",
      "File CT_Hartford_0.json processed. Rows: 42\n",
      "Processing file: MI_Lansing_3.json\n",
      "File MI_Lansing_3.json processed. Rows: 42\n",
      "Processing file: AL_Montgomery_0.json\n",
      "File AL_Montgomery_0.json processed. Rows: 42\n",
      "Processing file: ND_Bismarck_4.json\n",
      "No valid data in ND_Bismarck_4.json. Skipping...\n",
      "Processing file: TX_Austin_1.json\n",
      "File TX_Austin_1.json processed. Rows: 42\n",
      "Processing file: KS_Topeka_3.json\n",
      "File KS_Topeka_3.json processed. Rows: 9\n",
      "Processing file: CA_Sacramento_0.json\n",
      "File CA_Sacramento_0.json processed. Rows: 42\n",
      "Processing file: MO_JeffersonCity_3.json\n",
      "File MO_JeffersonCity_3.json processed. Rows: 18\n",
      "Processing file: MI_Lansing_2.json\n",
      "File MI_Lansing_2.json processed. Rows: 42\n",
      "Processing file: CO_Denver_4.json\n",
      "File CO_Denver_4.json processed. Rows: 42\n",
      "Processing file: MS_Jackson_1.json\n",
      "No valid data in MS_Jackson_1.json. Skipping...\n",
      "Processing file: VA_Richmond_0.json\n",
      "File VA_Richmond_0.json processed. Rows: 42\n",
      "Processing file: VA_Richmond_3.json\n",
      "File VA_Richmond_3.json processed. Rows: 42\n",
      "Processing file: OR_Salem_3.json\n",
      "File OR_Salem_3.json processed. Rows: 42\n",
      "Processing file: WY_Cheyenne_0.json\n",
      "No valid data in WY_Cheyenne_0.json. Skipping...\n",
      "Processing file: ID_Boise_4.json\n",
      "File ID_Boise_4.json processed. Rows: 42\n",
      "Processing file: CT_Hartford_4.json\n",
      "File CT_Hartford_4.json processed. Rows: 42\n",
      "Processing file: MN_St.Paul_0.json\n",
      "File MN_St.Paul_0.json processed. Rows: 42\n",
      "Processing file: NY_Albany_0.json\n",
      "File NY_Albany_0.json processed. Rows: 42\n",
      "Processing file: SC_Columbia_0.json\n",
      "File SC_Columbia_0.json processed. Rows: 34\n",
      "Processing file: NH_Concord_3.json\n",
      "No valid data in NH_Concord_3.json. Skipping...\n",
      "Processing file: LA_BatonRouge_3.json\n",
      "File LA_BatonRouge_3.json processed. Rows: 42\n",
      "Processing file: NJ_Trenton_1.json\n",
      "File NJ_Trenton_1.json processed. Rows: 42\n",
      "Processing file: HI_Honolulu_4.json\n",
      "No valid data in HI_Honolulu_4.json. Skipping...\n",
      "Processing file: AK_Juneau_3.json\n",
      "File AK_Juneau_3.json processed. Rows: 5\n",
      "Processing file: MA_Boston_3.json\n",
      "File MA_Boston_3.json processed. Rows: 42\n",
      "Processing file: OH_Columbus_0.json\n",
      "File OH_Columbus_0.json processed. Rows: 42\n",
      "Processing file: OH_Columbus_4.json\n",
      "File OH_Columbus_4.json processed. Rows: 42\n",
      "Processing file: WA_Olympia_3.json\n",
      "File WA_Olympia_3.json processed. Rows: 42\n",
      "Processing file: AR_LittleRock_2.json\n",
      "File AR_LittleRock_2.json processed. Rows: 42\n",
      "Processing file: MT_Helena_3.json\n",
      "File MT_Helena_3.json processed. Rows: 42\n",
      "Processing file: ID_Boise_0.json\n",
      "File ID_Boise_0.json processed. Rows: 42\n",
      "Processing file: MD_Annapolis_2.json\n",
      "File MD_Annapolis_2.json processed. Rows: 42\n",
      "Processing file: GA_Atlanta_0.json\n",
      "File GA_Atlanta_0.json processed. Rows: 42\n",
      "Processing file: LA_BatonRouge_2.json\n",
      "File LA_BatonRouge_2.json processed. Rows: 42\n",
      "Processing file: MT_Helena_0.json\n",
      "File MT_Helena_0.json processed. Rows: 42\n",
      "Processing file: KS_Topeka_1.json\n",
      "File KS_Topeka_1.json processed. Rows: 11\n",
      "Processing file: VA_Richmond_1.json\n",
      "File VA_Richmond_1.json processed. Rows: 42\n",
      "Processing file: TX_Austin_4.json\n",
      "File TX_Austin_4.json processed. Rows: 42\n",
      "Processing file: IA_DesMoines_0.json\n",
      "File IA_DesMoines_0.json processed. Rows: 42\n",
      "Processing file: IN_Indianapolis_2.json\n",
      "File IN_Indianapolis_2.json processed. Rows: 42\n",
      "Processing file: IN_Indianapolis_1.json\n",
      "File IN_Indianapolis_1.json processed. Rows: 42\n",
      "Processing file: MA_Boston_0.json\n",
      "File MA_Boston_0.json processed. Rows: 42\n",
      "Processing file: IL_Springfield_2.json\n",
      "File IL_Springfield_2.json processed. Rows: 35\n",
      "Processing file: SC_Columbia_1.json\n",
      "File SC_Columbia_1.json processed. Rows: 33\n",
      "Processing file: NJ_Trenton_2.json\n",
      "File NJ_Trenton_2.json processed. Rows: 42\n",
      "Processing file: OR_Salem_4.json\n",
      "File OR_Salem_4.json processed. Rows: 42\n",
      "Processing file: WI_Madison_3.json\n",
      "File WI_Madison_3.json processed. Rows: 42\n",
      "Processing file: ID_Boise_1.json\n",
      "File ID_Boise_1.json processed. Rows: 42\n",
      "Processing file: NV_CarsonCity_1.json\n",
      "File NV_CarsonCity_1.json processed. Rows: 42\n",
      "Processing file: ND_Bismarck_0.json\n",
      "File ND_Bismarck_0.json processed. Rows: 2\n",
      "Processing file: RI_Providence_4.json\n",
      "File RI_Providence_4.json processed. Rows: 42\n",
      "Processing file: AL_Montgomery_2.json\n",
      "File AL_Montgomery_2.json processed. Rows: 42\n",
      "Processing file: TX_Austin_3.json\n",
      "File TX_Austin_3.json processed. Rows: 42\n",
      "Processing file: OH_Columbus_2.json\n",
      "File OH_Columbus_2.json processed. Rows: 42\n",
      "Processing file: IA_DesMoines_1.json\n",
      "File IA_DesMoines_1.json processed. Rows: 42\n",
      "Processing file: HI_Honolulu_2.json\n",
      "File HI_Honolulu_2.json processed. Rows: 1\n",
      "Processing file: WV_Charleston_2.json\n",
      "File WV_Charleston_2.json processed. Rows: 41\n",
      "Processing file: FL_Tallahassee_4.json\n",
      "File FL_Tallahassee_4.json processed. Rows: 42\n",
      "Processing file: CO_Denver_0.json\n",
      "File CO_Denver_0.json processed. Rows: 42\n",
      "Processing file: AR_LittleRock_4.json\n",
      "File AR_LittleRock_4.json processed. Rows: 42\n",
      "Processing file: SD_Pierre_2.json\n",
      "No valid data in SD_Pierre_2.json. Skipping...\n",
      "Processing file: NY_Albany_2.json\n",
      "File NY_Albany_2.json processed. Rows: 42\n",
      "Processing file: HI_Honolulu_0.json\n",
      "File HI_Honolulu_0.json processed. Rows: 3\n",
      "Processing file: AZ_Phoenix_3.json\n",
      "File AZ_Phoenix_3.json processed. Rows: 42\n",
      "Processing file: RI_Providence_1.json\n",
      "File RI_Providence_1.json processed. Rows: 42\n",
      "Processing file: VT_Montpelier_3.json\n",
      "No valid data in VT_Montpelier_3.json. Skipping...\n",
      "Processing file: CT_Hartford_2.json\n",
      "File CT_Hartford_2.json processed. Rows: 42\n",
      "Processing file: AZ_Phoenix_2.json\n",
      "File AZ_Phoenix_2.json processed. Rows: 42\n",
      "Processing file: WA_Olympia_1.json\n",
      "File WA_Olympia_1.json processed. Rows: 42\n",
      "Processing file: NM_SantaFe_0.json\n",
      "File NM_SantaFe_0.json processed. Rows: 42\n",
      "Processing file: NC_Raleigh_4.json\n",
      "File NC_Raleigh_4.json processed. Rows: 42\n",
      "Processing file: IA_DesMoines_4.json\n",
      "File IA_DesMoines_4.json processed. Rows: 42\n",
      "Processing file: HI_Honolulu_1.json\n",
      "File HI_Honolulu_1.json processed. Rows: 2\n",
      "Processing file: NM_SantaFe_3.json\n",
      "File NM_SantaFe_3.json processed. Rows: 42\n",
      "Processing file: NC_Raleigh_1.json\n",
      "File NC_Raleigh_1.json processed. Rows: 42\n",
      "Processing file: NH_Concord_0.json\n",
      "File NH_Concord_0.json processed. Rows: 3\n",
      "Processing file: UT_SaltLakeCity_2.json\n",
      "File UT_SaltLakeCity_2.json processed. Rows: 42\n",
      "Processing file: NM_SantaFe_4.json\n",
      "File NM_SantaFe_4.json processed. Rows: 42\n",
      "Processing file: MS_Jackson_0.json\n",
      "No valid data in MS_Jackson_0.json. Skipping...\n",
      "Processing file: WI_Madison_4.json\n",
      "File WI_Madison_4.json processed. Rows: 42\n",
      "Processing file: MI_Lansing_0.json\n",
      "File MI_Lansing_0.json processed. Rows: 42\n"
     ]
    }
   ],
   "source": [
    "# loop over all files and put them into a dataframe# Iterate through each file in the folder\n",
    "for file in os.listdir(json_folder):\n",
    "    if file.endswith('.json'):  # Ensure it's a JSON file\n",
    "        file_path = os.path.join(json_folder, file)\n",
    "        print(f\"Processing file: {file}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            results = data.get('data', {}).get('results', [])\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"No valid data in {file}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Flatten the JSON structure for each file\n",
    "                df = pd.json_normalize(\n",
    "                    results,\n",
    "                    sep='_',\n",
    "                    meta=[\n",
    "                        'list_date',\n",
    "                        'list_price',\n",
    "                        'status',\n",
    "                        ['description', 'year_built'],\n",
    "                        ['description', 'sqft'],\n",
    "                        ['description', 'beds'],\n",
    "                        ['description', 'baths'],\n",
    "                        ['location', 'address', 'city'],\n",
    "                        ['location', 'address', 'state'],\n",
    "                        ['location', 'address', 'postal_code'],\n",
    "                    ],\n",
    "                    errors='ignore'\n",
    "                )\n",
    "                # Drop any columns that are all NaN\n",
    "                df.dropna(axis=1, how='all', inplace=True)\n",
    "                print(f\"File {file} processed. Rows: {len(df)}\")\n",
    "                data_frames.append(df)  # Add the DataFrame to the list\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, ensure that you have all sales in a dataframe.\n",
    "- Take a quick look at your data (i.e. `.info()`, `.describe()`) - what do you see?\n",
    "- Is each cell one value, or do some cells have lists?\n",
    "- What are the data types of each column?\n",
    "- Some sales may not actually include the sale price (target).  These rows should be dropped.\n",
    "- There are a lot of NA/None values.  Should these be dropped or replaced with something?\n",
    "    - You can drop rows or use various methods to fills NA's - use your best judgement for each column \n",
    "    - i.e. for some columns (like Garage), NA probably just means no Garage, so 0\n",
    "- Drop columns that aren't needed\n",
    "    - Don't keep the list price because it will be too close to the sale price. Assume we want to predict the price of houses not yet listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and concatenate data here\n",
    "# drop or replace values as necessary# Combine all DataFrames into one\n",
    "if data_frames:  \n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "else:\n",
    "    combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = combined_df.columns.sort_values().to_list()\n",
    "combined_df = combined_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop or replace values as necessary\n",
    "combined_df = combined_df.drop(\n",
    "    columns=[\n",
    "        'branding',\n",
    "        'community_advertisers',\n",
    "        'community_description_name',\n",
    "        # 'description.baths',\n",
    "        'description_baths_3qtr',\n",
    "        'description_baths_full',\n",
    "        'description_baths_half',\n",
    "        # 'description.beds',\n",
    "        # 'description.garage',\n",
    "        # 'description.lot_sqft',\n",
    "        # 'description.sold_date',\n",
    "        # 'description.sold_price',\n",
    "        # 'description.sqft',\n",
    "        # 'description.stories',\n",
    "        'description_sub_type',\n",
    "        'description_type',\n",
    "        # 'description.year_built',\n",
    "         'flags_is_foreclosure',\n",
    "         'flags_is_new_listing',\n",
    "         'flags_is_price_reduced',\n",
    "        'last_update_date',\n",
    "        'lead_attributes_show_contact_an_agent',\n",
    "        # 'list_date',\n",
    "        # 'list_price',\n",
    "        'listing_id',\n",
    "        # 'location.address.city',\n",
    "         'location_address_coordinate_lat',\n",
    "         'location_address_coordinate_lon',\n",
    "        'location_address_line',\n",
    "        # 'location.address.postal_code',\n",
    "        # 'location.address.state',\n",
    "         'location_address_state_code',\n",
    "        # 'location.county.fips_code',\n",
    "        # 'location.county.name',\n",
    "        'location_street_view_url',\n",
    "        'matterport',\n",
    "        'other_listings_rdc',\n",
    "        'permalink',\n",
    "        'photos',\n",
    "        # 'price_reduced_amount',\n",
    "        'primary_photo_href',\n",
    "        '#products_brand_name',\n",
    "        # 'property_id',\n",
    "        'source_agents',\n",
    "        'source_plan_id',\n",
    "        'source_spec_id',\n",
    "        'source_type',\n",
    "        'status',\n",
    "        # 'tags',\n",
    "        'virtual_tours'\n",
    "    ],\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the rows that dont have a listing price\n",
    "combined_df = combined_df.dropna(subset=['list_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 0 to all the rows that have description_garage as NaN\n",
    "combined_df['description_garage'] = combined_df['description_garage'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "description_stories\n",
       "1.0    3224\n",
       "2.0    2449\n",
       "3.0     432\n",
       "4.0      25\n",
       "6.0      16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check and count the different values in the column description_stories\n",
    "combined_df['description_stories'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description_sold_price\n",
      "308.0          5\n",
      "385.0          5\n",
      "583.0          5\n",
      "1122.0        10\n",
      "2500.0         5\n",
      "2585.0         5\n",
      "3575.0         5\n",
      "3800.0         5\n",
      "4500.0         5\n",
      "5000.0         5\n",
      "5900.0         5\n",
      "6380.0         5\n",
      "6930.0         5\n",
      "7590.0         5\n",
      "7700.0         5\n",
      "9000.0         5\n",
      "9900.0         5\n",
      "10000.0        5\n",
      "12900.0        3\n",
      "13200.0        2\n",
      "14300.0        5\n",
      "14900.0        5\n",
      "15400.0       10\n",
      "15900.0        5\n",
      "16500.0        5\n",
      "18900.0        5\n",
      "19900.0        5\n",
      "20700.0        5\n",
      "20900.0        5\n",
      "22000.0       10\n",
      "25000.0       13\n",
      "26000.0        5\n",
      "27225.0        5\n",
      "27500.0        5\n",
      "28000.0       15\n",
      "28600.0        5\n",
      "29700.0       10\n",
      "29900.0        5\n",
      "30000.0       10\n",
      "31500.0        5\n",
      "32500.0        1\n",
      "32900.0        6\n",
      "35000.0        5\n",
      "38500.0        3\n",
      "39900.0        5\n",
      "40000.0        5\n",
      "41800.0        5\n",
      "42000.0        5\n",
      "42900.0       10\n",
      "45000.0        5\n",
      "48500.0        5\n",
      "49900.0        5\n",
      "50000.0        9\n",
      "51860.0        5\n",
      "53325.0        5\n",
      "53591.0        5\n",
      "54167.0        5\n",
      "55000.0        5\n",
      "56000.0        4\n",
      "57000.0        5\n",
      "57054.0        5\n",
      "58000.0        5\n",
      "58075.0        5\n",
      "59000.0        5\n",
      "59400.0        5\n",
      "60000.0       25\n",
      "61050.0        4\n",
      "62000.0        5\n",
      "62500.0        5\n",
      "63000.0       10\n",
      "63500.0        5\n",
      "63800.0        5\n",
      "64000.0        3\n",
      "64975.0        5\n",
      "65000.0       16\n",
      "68000.0       10\n",
      "68250.0        5\n",
      "68750.0        5\n",
      "69000.0       10\n",
      "69500.0        5\n",
      "71500.0        5\n",
      "74000.0        4\n",
      "74250.0        5\n",
      "75000.0       28\n",
      "76000.0        5\n",
      "79000.0        5\n",
      "79900.0        5\n",
      "85000.0       15\n",
      "86000.0        5\n",
      "86500.0        5\n",
      "88000.0        5\n",
      "88500.0        2\n",
      "89500.0        5\n",
      "89900.0       10\n",
      "90000.0        5\n",
      "91000.0        5\n",
      "92500.0        5\n",
      "93950.0        5\n",
      "94500.0        5\n",
      "95000.0        5\n",
      "95900.0        5\n",
      "97000.0        5\n",
      "99000.0        5\n",
      "99100.0        1\n",
      "99275.0        5\n",
      "100000.0      30\n",
      "102000.0      10\n",
      "103000.0       5\n",
      "105000.0      11\n",
      "106000.0      20\n",
      "106900.0       5\n",
      "107000.0      10\n",
      "107500.0       5\n",
      "108000.0       3\n",
      "109000.0       5\n",
      "110000.0       7\n",
      "111200.0       5\n",
      "112000.0       5\n",
      "114000.0       5\n",
      "114580.0       5\n",
      "115000.0      32\n",
      "115500.0       9\n",
      "117000.0       2\n",
      "117777.0       5\n",
      "118000.0       9\n",
      "120000.0      30\n",
      "122000.0       4\n",
      "122500.0       5\n",
      "123000.0       5\n",
      "125000.0      28\n",
      "127000.0       2\n",
      "128500.0       1\n",
      "128700.0       5\n",
      "129900.0       1\n",
      "130000.0       9\n",
      "132000.0       1\n",
      "132500.0       3\n",
      "133000.0       5\n",
      "134000.0       5\n",
      "135000.0      25\n",
      "135500.0       1\n",
      "135900.0       2\n",
      "136000.0      13\n",
      "137000.0       5\n",
      "138000.0      10\n",
      "139900.0       5\n",
      "140000.0      23\n",
      "140267.0       3\n",
      "142000.0       3\n",
      "143500.0       5\n",
      "144000.0      10\n",
      "144815.0       5\n",
      "145000.0      35\n",
      "147000.0      15\n",
      "147500.0       5\n",
      "148000.0       2\n",
      "148800.0       5\n",
      "149900.0      10\n",
      "150000.0      38\n",
      "151000.0       5\n",
      "152500.0       5\n",
      "153000.0      15\n",
      "154000.0       8\n",
      "154900.0       2\n",
      "155000.0      25\n",
      "155350.0       5\n",
      "157000.0       5\n",
      "158000.0      15\n",
      "159000.0      10\n",
      "159900.0      15\n",
      "160000.0      25\n",
      "161000.0       1\n",
      "161500.0       5\n",
      "162000.0       9\n",
      "162250.0       5\n",
      "162450.0       5\n",
      "162500.0       5\n",
      "163000.0      13\n",
      "164900.0       5\n",
      "165000.0      28\n",
      "165900.0       5\n",
      "167000.0       5\n",
      "168000.0       5\n",
      "169000.0       5\n",
      "169400.0       5\n",
      "169500.0       5\n",
      "169900.0       9\n",
      "170000.0      15\n",
      "172000.0      10\n",
      "172500.0      15\n",
      "172900.0       5\n",
      "173000.0       2\n",
      "174632.0       5\n",
      "175000.0       5\n",
      "175100.0       5\n",
      "175550.0       5\n",
      "177500.0       5\n",
      "178000.0      10\n",
      "179000.0       5\n",
      "180000.0      28\n",
      "184900.0       5\n",
      "185000.0      44\n",
      "186000.0       5\n",
      "187000.0       5\n",
      "188000.0      15\n",
      "189000.0       5\n",
      "190000.0      25\n",
      "191000.0       5\n",
      "191500.0       5\n",
      "194628.0       5\n",
      "194900.0       5\n",
      "195000.0      20\n",
      "196000.0       2\n",
      "197000.0      10\n",
      "199900.0       5\n",
      "200000.0      66\n",
      "200791.0       5\n",
      "202000.0       3\n",
      "203000.0      15\n",
      "204000.0       5\n",
      "205000.0      25\n",
      "205500.0       5\n",
      "206000.0       5\n",
      "207000.0      10\n",
      "208000.0       5\n",
      "209000.0       5\n",
      "209500.0       5\n",
      "210000.0      31\n",
      "211000.0       3\n",
      "212000.0       5\n",
      "214000.0      10\n",
      "214400.0       5\n",
      "215000.0      22\n",
      "217000.0       5\n",
      "218829.0       5\n",
      "219000.0      10\n",
      "219900.0       5\n",
      "220000.0      63\n",
      "221000.0      10\n",
      "221500.0       5\n",
      "222000.0      10\n",
      "223000.0       5\n",
      "224325.0       5\n",
      "224825.0       5\n",
      "224900.0       5\n",
      "225000.0      56\n",
      "225900.0       5\n",
      "226000.0       2\n",
      "226500.0       5\n",
      "227000.0      10\n",
      "227500.0       5\n",
      "228000.0       5\n",
      "229000.0       5\n",
      "229900.0       2\n",
      "229995.0       5\n",
      "230000.0      36\n",
      "231000.0       5\n",
      "231650.0       2\n",
      "232000.0      13\n",
      "234000.0       5\n",
      "234900.0       5\n",
      "235000.0      23\n",
      "235001.0       5\n",
      "236000.0      10\n",
      "236500.0       5\n",
      "237000.0       5\n",
      "237050.0       5\n",
      "238000.0      10\n",
      "239000.0       9\n",
      "239900.0       5\n",
      "240000.0      40\n",
      "240300.0       5\n",
      "241000.0       5\n",
      "241500.0       5\n",
      "242000.0       5\n",
      "243000.0       3\n",
      "244500.0       5\n",
      "245000.0      47\n",
      "246500.0       5\n",
      "247000.0       5\n",
      "247200.0       1\n",
      "247400.0       5\n",
      "247900.0       5\n",
      "248000.0       5\n",
      "249900.0       5\n",
      "250000.0      52\n",
      "252000.0       2\n",
      "253000.0       7\n",
      "255000.0      26\n",
      "255900.0       5\n",
      "258000.0      11\n",
      "259000.0       5\n",
      "259500.0       5\n",
      "259900.0      12\n",
      "260000.0      50\n",
      "261000.0       5\n",
      "261250.0       5\n",
      "261900.0       3\n",
      "262500.0      15\n",
      "263000.0       6\n",
      "263626.0       5\n",
      "264900.0       5\n",
      "265000.0      25\n",
      "265300.0       3\n",
      "266000.0       5\n",
      "266900.0       5\n",
      "267000.0       5\n",
      "267500.0       5\n",
      "269000.0       9\n",
      "270000.0      35\n",
      "271000.0       6\n",
      "271500.0       5\n",
      "272000.0       5\n",
      "272500.0       8\n",
      "274000.0       5\n",
      "274500.0       5\n",
      "275000.0      27\n",
      "276000.0       6\n",
      "277000.0       5\n",
      "277875.0       5\n",
      "278000.0       5\n",
      "279000.0       7\n",
      "280000.0      47\n",
      "281000.0       5\n",
      "282000.0      10\n",
      "283000.0      15\n",
      "283500.0       5\n",
      "284500.0      10\n",
      "284900.0       5\n",
      "285000.0      28\n",
      "287000.0       5\n",
      "287500.0       5\n",
      "288000.0       5\n",
      "289000.0       5\n",
      "289900.0       5\n",
      "290000.0      16\n",
      "292000.0      10\n",
      "292500.0       5\n",
      "293000.0       5\n",
      "293500.0       5\n",
      "294000.0       2\n",
      "295000.0      38\n",
      "297000.0      10\n",
      "298800.0       5\n",
      "298897.0       3\n",
      "299000.0      10\n",
      "299500.0       2\n",
      "300000.0      59\n",
      "302500.0       3\n",
      "303000.0       5\n",
      "304000.0       1\n",
      "305000.0      32\n",
      "306000.0       5\n",
      "307000.0      10\n",
      "309900.0       5\n",
      "310000.0      34\n",
      "311500.0       3\n",
      "312000.0      10\n",
      "313000.0       5\n",
      "314000.0       5\n",
      "315000.0      55\n",
      "318000.0      14\n",
      "319999.0       2\n",
      "320000.0      42\n",
      "322000.0       5\n",
      "323000.0      10\n",
      "324000.0       5\n",
      "324900.0       5\n",
      "325000.0      45\n",
      "326000.0       5\n",
      "327000.0       5\n",
      "327500.0       5\n",
      "328350.0       5\n",
      "328500.0       5\n",
      "330000.0      44\n",
      "331000.0       5\n",
      "331500.0       5\n",
      "332000.0       5\n",
      "332500.0       5\n",
      "333500.0       3\n",
      "333900.0       5\n",
      "335000.0      36\n",
      "335500.0       2\n",
      "339000.0      12\n",
      "340000.0      40\n",
      "343000.0       9\n",
      "345000.0      45\n",
      "346000.0       5\n",
      "347000.0      10\n",
      "349000.0       5\n",
      "349500.0       5\n",
      "349900.0       2\n",
      "350000.0      62\n",
      "350500.0       5\n",
      "354900.0       5\n",
      "355000.0      17\n",
      "356000.0       5\n",
      "357000.0       5\n",
      "358000.0       5\n",
      "359000.0       5\n",
      "359500.0       3\n",
      "359900.0      10\n",
      "360000.0      38\n",
      "362000.0       5\n",
      "362500.0       5\n",
      "364500.0       5\n",
      "364900.0       5\n",
      "365000.0      20\n",
      "366000.0       5\n",
      "368000.0      10\n",
      "369900.0       5\n",
      "370000.0      29\n",
      "370512.0       3\n",
      "371000.0       5\n",
      "371500.0       5\n",
      "372500.0       5\n",
      "373000.0       5\n",
      "374000.0      10\n",
      "375000.0      46\n",
      "376500.0       5\n",
      "379000.0       5\n",
      "379990.0       5\n",
      "380000.0      28\n",
      "382000.0       2\n",
      "383000.0       5\n",
      "385000.0      30\n",
      "387900.0       5\n",
      "388000.0       6\n",
      "389500.0       5\n",
      "389900.0      10\n",
      "390000.0      22\n",
      "392000.0       4\n",
      "393000.0       3\n",
      "395000.0      33\n",
      "396500.0       5\n",
      "399000.0       5\n",
      "399900.0       5\n",
      "399950.0       5\n",
      "400000.0      38\n",
      "401000.0       5\n",
      "402000.0       3\n",
      "405000.0      21\n",
      "410000.0      35\n",
      "410500.0       5\n",
      "412000.0      20\n",
      "415000.0      50\n",
      "417000.0       5\n",
      "417500.0       5\n",
      "419000.0       5\n",
      "420000.0      27\n",
      "421000.0       5\n",
      "421480.0       1\n",
      "422500.0       5\n",
      "424099.0       5\n",
      "424900.0       5\n",
      "425000.0      48\n",
      "427900.0       5\n",
      "428000.0       5\n",
      "429000.0      10\n",
      "430000.0      50\n",
      "431000.0       5\n",
      "432000.0       5\n",
      "433000.0       5\n",
      "434500.0       5\n",
      "435000.0      11\n",
      "436000.0       5\n",
      "437000.0       5\n",
      "439000.0       5\n",
      "439900.0       5\n",
      "440000.0      30\n",
      "441000.0       5\n",
      "442015.0       5\n",
      "445000.0       6\n",
      "448000.0       5\n",
      "448700.0       5\n",
      "449000.0       5\n",
      "450000.0      48\n",
      "452000.0       3\n",
      "452500.0       5\n",
      "453990.0       5\n",
      "454000.0       5\n",
      "455000.0      10\n",
      "455555.0       5\n",
      "457000.0       3\n",
      "459900.0       5\n",
      "460000.0      25\n",
      "465000.0      35\n",
      "466000.0       5\n",
      "467500.0       3\n",
      "468000.0       5\n",
      "469000.0       1\n",
      "470000.0      20\n",
      "472000.0       8\n",
      "472500.0       5\n",
      "475000.0      22\n",
      "480000.0      11\n",
      "485000.0      39\n",
      "487000.0      15\n",
      "490000.0      27\n",
      "492450.0       2\n",
      "494000.0       5\n",
      "495000.0      15\n",
      "496000.0       5\n",
      "497000.0       5\n",
      "497900.0       5\n",
      "498000.0       5\n",
      "498500.0       5\n",
      "500000.0      15\n",
      "500100.0       5\n",
      "505000.0      20\n",
      "509000.0       5\n",
      "510000.0      10\n",
      "512000.0       4\n",
      "512500.0       2\n",
      "513000.0       4\n",
      "515000.0      27\n",
      "518444.0       5\n",
      "519000.0       5\n",
      "519450.0       5\n",
      "520000.0      16\n",
      "522500.0       5\n",
      "524950.0       5\n",
      "525000.0      21\n",
      "527000.0       5\n",
      "527500.0       3\n",
      "529900.0       5\n",
      "530000.0      30\n",
      "534000.0       5\n",
      "539900.0       5\n",
      "540000.0      15\n",
      "541000.0       5\n",
      "542500.0       5\n",
      "545000.0       5\n",
      "545900.0       5\n",
      "550000.0      30\n",
      "555000.0      20\n",
      "556000.0       5\n",
      "558000.0       5\n",
      "559900.0       5\n",
      "560000.0      15\n",
      "562000.0       5\n",
      "565000.0      10\n",
      "570000.0      30\n",
      "575000.0      23\n",
      "580000.0      10\n",
      "585000.0      15\n",
      "589000.0       5\n",
      "590000.0      10\n",
      "592790.0       1\n",
      "595000.0       5\n",
      "599000.0      15\n",
      "599900.0       5\n",
      "600000.0      10\n",
      "607900.0       5\n",
      "609000.0       5\n",
      "610000.0      15\n",
      "615000.0      10\n",
      "619000.0       5\n",
      "620000.0       5\n",
      "621000.0       5\n",
      "623000.0       5\n",
      "625000.0       5\n",
      "625600.0       5\n",
      "629000.0       5\n",
      "630000.0       6\n",
      "632500.0       5\n",
      "635000.0       8\n",
      "637500.0      10\n",
      "640000.0       6\n",
      "641450.0       5\n",
      "642500.0       5\n",
      "644000.0       3\n",
      "645000.0       5\n",
      "650000.0      10\n",
      "651000.0       5\n",
      "655000.0       4\n",
      "660000.0       5\n",
      "665000.0       6\n",
      "667500.0       4\n",
      "669000.0       1\n",
      "670000.0      10\n",
      "675000.0      26\n",
      "685000.0       5\n",
      "689000.0       3\n",
      "689900.0       5\n",
      "689999.0       5\n",
      "690000.0      10\n",
      "698000.0       5\n",
      "699000.0       5\n",
      "700000.0      24\n",
      "703125.0       5\n",
      "705000.0      16\n",
      "707000.0       5\n",
      "707500.0       5\n",
      "709900.0       1\n",
      "710000.0      10\n",
      "718000.0       2\n",
      "718750.0       5\n",
      "719000.0       5\n",
      "725000.0       5\n",
      "730000.0       5\n",
      "733900.0       5\n",
      "737000.0       5\n",
      "739900.0       5\n",
      "740000.0       4\n",
      "745000.0       9\n",
      "747000.0       4\n",
      "750000.0      20\n",
      "755000.0       9\n",
      "759000.0       5\n",
      "760000.0       7\n",
      "762000.0       5\n",
      "765000.0       5\n",
      "770000.0       5\n",
      "775000.0       5\n",
      "777000.0       5\n",
      "779000.0       5\n",
      "780000.0       5\n",
      "784000.0       5\n",
      "785000.0       5\n",
      "789900.0       5\n",
      "800000.0      15\n",
      "810000.0      15\n",
      "820000.0      12\n",
      "822500.0      10\n",
      "823500.0       5\n",
      "825000.0      10\n",
      "826000.0       5\n",
      "827522.0       5\n",
      "850000.0       5\n",
      "858000.0       5\n",
      "874900.0       5\n",
      "875000.0       3\n",
      "891000.0       5\n",
      "895000.0       5\n",
      "900000.0       5\n",
      "903000.0       5\n",
      "905000.0       5\n",
      "910000.0       5\n",
      "925000.0       1\n",
      "950000.0       5\n",
      "960000.0       5\n",
      "980000.0       5\n",
      "985000.0       5\n",
      "988750.0       1\n",
      "999999.0       7\n",
      "1000000.0     10\n",
      "1010000.0      5\n",
      "1038712.0      5\n",
      "1050000.0      5\n",
      "1075000.0     10\n",
      "1090000.0      5\n",
      "1100000.0      9\n",
      "1125000.0      9\n",
      "1150000.0     10\n",
      "1159000.0      5\n",
      "1170000.0      5\n",
      "1180000.0      6\n",
      "1195000.0      5\n",
      "1200000.0      5\n",
      "1206000.0      3\n",
      "1220000.0      3\n",
      "1222000.0      1\n",
      "1244104.0      5\n",
      "1250000.0      5\n",
      "1279000.0      5\n",
      "1285000.0      5\n",
      "1299000.0      5\n",
      "1325000.0      5\n",
      "1375000.0      5\n",
      "1400000.0      5\n",
      "1410000.0     15\n",
      "1437500.0      5\n",
      "1450000.0      1\n",
      "1460000.0      5\n",
      "1465000.0      2\n",
      "1500000.0     13\n",
      "1550000.0      5\n",
      "1560000.0      5\n",
      "1660000.0      5\n",
      "1700000.0      5\n",
      "1701500.0      5\n",
      "1760000.0      5\n",
      "1795000.0      5\n",
      "1862500.0      5\n",
      "1900000.0      5\n",
      "1915000.0      5\n",
      "1925000.0      5\n",
      "2016000.0      7\n",
      "2100000.0      5\n",
      "2125000.0      5\n",
      "2300000.0      5\n",
      "2500000.0      5\n",
      "2600000.0      5\n",
      "2800000.0      9\n",
      "3200000.0      5\n",
      "3999999.0      5\n",
      "7400000.0      5\n",
      "12500000.0     5\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6.422000e+03\n",
       "mean     3.964653e+05\n",
       "std      5.215674e+05\n",
       "min      3.080000e+02\n",
       "25%      1.900000e+05\n",
       "50%      3.100000e+05\n",
       "75%      4.600000e+05\n",
       "max      1.250000e+07\n",
       "Name: description_sold_price, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count values on the column description_sold_price and put them in ascending order set option to display all the rows\n",
    "#combined_df['description_sold_price'].value_counts().sort_index(ascending=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "print(combined_df['description_sold_price'].value_counts().sort_index(ascending=True))\n",
    "\n",
    "#check for outliers in the column description_sold_price\n",
    "combined_df['description_sold_price'].describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n",
      "description_beds\n",
      "3.0     586\n",
      "4.0     266\n",
      "2.0     234\n",
      "5.0      69\n",
      "1.0      39\n",
      "6.0      21\n",
      "8.0       5\n",
      "9.0       5\n",
      "12.0      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for outliers in the column list_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the rows that have description_stories = 10.0, 8.0\n",
    "combined_df = combined_df[~combined_df['description_stories'].isin([10.0, 8.0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7714 entries, 1073 to 8074\n",
      "Data columns (total 20 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   description_baths             7569 non-null   float64\n",
      " 1   description_beds              7376 non-null   float64\n",
      " 2   description_garage            7714 non-null   float64\n",
      " 3   description_lot_sqft          6839 non-null   float64\n",
      " 4   description_sold_date         7714 non-null   object \n",
      " 5   description_sold_price        6422 non-null   float64\n",
      " 6   description_sqft              7214 non-null   float64\n",
      " 7   description_stories           6146 non-null   float64\n",
      " 8   description_year_built        7199 non-null   float64\n",
      " 9   list_date                     7714 non-null   object \n",
      " 10  list_price                    7714 non-null   float64\n",
      " 11  location_address_city         7709 non-null   object \n",
      " 12  location_address_postal_code  7714 non-null   object \n",
      " 13  location_address_state        7714 non-null   object \n",
      " 14  location_county_fips_code     7268 non-null   object \n",
      " 15  location_county_name          7704 non-null   object \n",
      " 16  price_reduced_amount          2484 non-null   float64\n",
      " 17  products_brand_name           7635 non-null   object \n",
      " 18  property_id                   7714 non-null   object \n",
      " 19  tags                          7600 non-null   object \n",
      "dtypes: float64(10), object(10)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the fact that with tags, there are a lot of categorical variables.\n",
    "- How many columns would we have if we OHE tags, city and state?\n",
    "- Perhaps we can get rid of tags that have a low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a CSV file\n",
    "output_file = 'combined_cleaned_data.csv'\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE categorical variables/ tags here\n",
    "# tags will have to be done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sales will vary drastically between cities and states.  Is there a way to keep information about which city it is without OHE?\n",
    "- Could we label encode or ordinal encode?  Yes, but this may have undesirable effects, giving nominal data ordinal values.\n",
    "- What we can do is use our training data to encode the mean sale price by city as a feature (a.k.a. Target Encoding)\n",
    "    - We can do this as long as we ONLY use the training data - we're using the available data to give us a 'starting guess' of the price for each city, without needing to encode city explicitly\n",
    "- If you replace cities or states with numerical values (like the mean price), make sure that the data is split so that we don't leak data into the training selection. This is a great time to train test split. Compute on the training data, and join these values to the test data\n",
    "- Note that you *may* have cities in the test set that are not in the training set. You don't want these to be NA, so maybe you can fill them with the overall mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split here\n",
    "\n",
    "# do something with state and city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data - STRETCH\n",
    "\n",
    "> This doesn't need to be part of your Minimum Viable Product (MVP). We recommend you write a functional, basic pipeline first, then circle back and join new data if you have time\n",
    "\n",
    "> If you do this, try to write your downstream steps in a way it will still work on a dataframe with different features!\n",
    "\n",
    "- You're not limited to just using the data provided to you. Think/ do some research about other features that might be useful to predict housing prices. \n",
    "- Can you import and join this data? Make sure you do any necessary preprocessing and make sure it is joined correctly.\n",
    "- Example suggestion: could mortgage interest rates in the year of the listing affect the price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, join and preprocess new data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA/ Visualization\n",
    "\n",
    "Remember all of the EDA that you've been learning about?  Now is a perfect time for it!\n",
    "- Look at distributions of numerical variables to see the shape of the data and detect outliers.    \n",
    "    - Consider transforming very skewed variables\n",
    "- Scatterplots of a numerical variable and the target go a long way to show correlations.\n",
    "- A heatmap will help detect highly correlated features, and we don't want these.\n",
    "    - You may have too many features to do this, in which case you can simply compute the most correlated feature-pairs and list them\n",
    "- Is there any overlap in any of the features? (redundant information, like number of this or that room...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Finishing Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a great time to scale the data and save it once it's preprocessed.\n",
    "- You can save it in your data folder, but you may want to make a new `processed/` subfolder to keep it organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
